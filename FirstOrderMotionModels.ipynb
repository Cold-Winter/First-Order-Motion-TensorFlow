{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FirstOrderMotionModels.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hoDTEF2sIIAh",
        "DNgZBFUa9Yyu",
        "JNbVT8HN9dYf",
        "VQ24aagI9hUz",
        "A_1v20SyaBIz",
        "oTOkkc26NK_Q",
        "-ZxKSxK81j1V",
        "PX4dE0URILvJ",
        "4vdaO9Y4qXTP",
        "XWIEmkHkjLyY",
        "OTy4r-I0yQaY",
        "KCnkTEGXM7bh",
        "eAEsh_H5aBH1",
        "UD9NtbBgOgdZ",
        "xO71ZAKwEcGM",
        "bmGbXzFIoPfV",
        "CPL_vdUDP9uj",
        "8u3trc5ODSMn",
        "m-eMVlXJemd0",
        "-NG8sfnoFS2O"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO2-KaAGH4wk",
        "colab_type": "code",
        "outputId": "c90054f2-e41f-40af-bc14-43c6f5b3d2ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!pip install tfa-nightly"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tfa-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/0e/c1ce899fac16a86ed7a153270673f27442cff4f543232dd0f4a187294a13/tfa_nightly-0.10.0.dev20200509080841-cp36-cp36m-manylinux2010_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.6/dist-packages (from tfa-nightly) (2.7.1)\n",
            "Installing collected packages: tfa-nightly\n",
            "Successfully installed tfa-nightly-0.10.0.dev20200509080841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ5C9ZxQH5FI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUQO5SASxznr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl_yPfzYx-jf",
        "colab_type": "code",
        "outputId": "c8d1d73e-4457-49be-b75a-da647b076e89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdwY217mx_vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W7-bLwkyEZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAFxSyGU_cyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoDTEF2sIIAh",
        "colab_type": "text"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi_GnqnHlI-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detach_keypoint(keypoint):\n",
        "  return { key: tf.stop_gradient(value) for key, value in keypoint.items() }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNgZBFUa9Yyu",
        "colab_type": "text"
      },
      "source": [
        "## Blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSK1EKDQdcVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SameBlock2d(tf.keras.Model):\n",
        "  def __init__(self, num_features):\n",
        "    super(SameBlock2d, self).__init__()\n",
        "    self.padding = [[0, 0], [1, 1], [1, 1], [0, 0]]\n",
        "    self.conv = layers.Conv2D(num_features, (3, 3), strides=1, padding=[1, 1], use_bias=False)\n",
        "    self.batch_norm = layers.BatchNormalization()\n",
        "  \n",
        "  def call(self, input_layer):\n",
        "    block = self.conv(input_layer)\n",
        "    block = self.batch_norm(block)\n",
        "    block = layers.ReLU()(block)\n",
        "\n",
        "    return block"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu9UFZu4gvIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResBlock2d(tf.keras.Model):\n",
        "  def __init__(self, num_features):\n",
        "    super(ResBlock2d, self).__init__()\n",
        "    self.conv1 = layers.Conv2D(num_features, (3, 3), strides=1, padding=[1, 1], use_bias=False)\n",
        "    self.conv2 = layers.Conv2D(num_features, (3, 3), strides=1, padding=[1, 1], use_bias=False)\n",
        "    self.batch_norm1 = layers.BatchNormalization()\n",
        "    self.batch_norm2 = layers.BatchNormalization()\n",
        "  \n",
        "  def call(self, input_layer):\n",
        "    block = self.batch_norm1(input_layer)\n",
        "    block = layers.ReLU()(block)\n",
        "    block = self.conv1(block)\n",
        "    block = self.batch_norm2(block)\n",
        "    block = layers.ReLU()(block)\n",
        "    block = self.conv2(block)\n",
        "\n",
        "    block += input_layer\n",
        "\n",
        "    return block"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHdjGEhKROvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DownBlock2d(tf.keras.Model):\n",
        "  def __init__(self, num_features, norm, pool):\n",
        "    super(DownBlock2d, self).__init__()\n",
        "    self.norm = norm\n",
        "    self.pool = pool\n",
        "    self.conv = layers.Conv2D(num_features, (4, 4), strides=1, padding=\"valid\")\n",
        "    # compare this with nn.InstanceNorm2d of pytorch using a conv2d with the same weights in both frameworks\n",
        "    self.instance_norm = tfa.layers.InstanceNormalization(axis=3)\n",
        "  \n",
        "  def call(self, input_layer):\n",
        "    block = self.conv(input_layer)\n",
        "    if self.norm:\n",
        "      block = self.instance_norm(block)\n",
        "    block = layers.ReLU()(block)\n",
        "    if self.pool:\n",
        "      block = layers.AveragePooling2D()(block)\n",
        "\n",
        "    return block"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNbVT8HN9dYf",
        "colab_type": "text"
      },
      "source": [
        "## Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQPgsWglGK0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def interpolate_tensor(tensor_input, final_shape):\n",
        "  original_shape = tensor_input.shape[1]\n",
        "\n",
        "  if final_shape > original_shape:\n",
        "    return interpolate_increase_size(tensor_input, final_shape)\n",
        "  else:\n",
        "    return interpolate_reduce_size(tensor_input, final_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ24aagI9hUz",
        "colab_type": "text"
      },
      "source": [
        "### Add size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYO8O5Lt9cwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def interpolate_increase_size(tensor_input, final_shape):\n",
        "  original_shape = tensor_input.shape[1]\n",
        "  difference = final_shape - original_shape\n",
        "  border = difference / 2\n",
        "\n",
        "  padding = [[0, 0], [int(border), int(border)], [int(border), int(border)], [0, 0]]\n",
        "\n",
        "  width = final_shape\n",
        "  height = final_shape\n",
        "  x = tf.linspace(border, width - (border + 1), width)\n",
        "  yy = tf.tile(tf.reshape(x, (-1, 1)), [1, width])\n",
        "  xx = tf.tile(tf.reshape(x, (1, -1)), [height, 1])\n",
        "\n",
        "  grid = tf.concat([tf.expand_dims(xx, axis=2), tf.expand_dims(yy, axis=2)], axis=2)\n",
        "\n",
        "  output = tfa.image.resampler(tf.pad(tensor_input, padding), tf.expand_dims(grid, axis=0))\n",
        "\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLSPib4hFZS8",
        "colab_type": "text"
      },
      "source": [
        "### Reduce size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC64v4Qs9o86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def interpolate_reduce_size(tensor_input, final_shape):\n",
        "  original_shape = tensor_input.shape[1]\n",
        "  width = original_shape\n",
        "  height = original_shape\n",
        "  x = tf.linspace(0.0, width - 1, final_shape)\n",
        "\n",
        "  yy = tf.tile(tf.reshape(x, (-1, 1)), [1, final_shape])\n",
        "  xx = tf.tile(tf.reshape(x, (1, -1)), [final_shape, 1])\n",
        "\n",
        "  grid = tf.concat([tf.expand_dims(xx, axis=2), tf.expand_dims(yy, axis=2)], axis=2)\n",
        "\n",
        "  output = tfa.image.resampler(tensor_input, tf.expand_dims(grid, axis=0))\n",
        "\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_1v20SyaBIz",
        "colab_type": "text"
      },
      "source": [
        "## Anti Aliasing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrORA77naAkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AntiAliasInterpolation(tf.keras.Model):\n",
        "  def __init__(self, channels, scale):\n",
        "    super(AntiAliasInterpolation, self).__init__()\n",
        "    sigma = (1 / scale - 1) / 2\n",
        "    kernel_size = 2 * round(sigma * 4) + 1\n",
        "    self.scale = scale\n",
        "\n",
        "    kernel_size = [kernel_size, kernel_size]\n",
        "    sigma = [sigma, sigma]\n",
        "\n",
        "    kernel = 1\n",
        "    meshgrids = tf.meshgrid(*[tf.keras.backend.arange(size, dtype='float32') for size in kernel_size], indexing='ij')\n",
        "\n",
        "    for size, std, mgrid in zip(kernel_size, sigma, meshgrids):\n",
        "      mean = (size - 1) / 2\n",
        "      kernel *= tf.math.exp(-(mgrid - mean) ** 2 / (2 * std ** 2))\n",
        "\n",
        "    kernel = kernel / tf.keras.backend.sum(kernel)\n",
        "\n",
        "    #[kernel_height, kernel_width, channels, num_kernels))]\n",
        "    kernel = tf.reshape(kernel, [*kernel.shape, 1, 1])\n",
        "    kernel = tf.tile(kernel, [1, 1, channels, 1])\n",
        "\n",
        "    # Important since we want to apply the kernel to each channel dimension\n",
        "    self.conv = layers.DepthwiseConv2D(kernel_size=kernel_size, strides=1, use_bias=False, padding=\"same\", weights=[kernel])\n",
        "\n",
        "  def call(self, input):\n",
        "    if self.scale == 1.0:\n",
        "      return input\n",
        "\n",
        "    out = self.conv(input)\n",
        "    new_size = int(self.scale * input.shape[1])\n",
        "    out = interpolate_tensor(out, new_size)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTOkkc26NK_Q",
        "colab_type": "text"
      },
      "source": [
        "## Image Pyramide"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV0buOa_NKJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImagePyramide(tf.keras.Model):\n",
        "  def __init__(self, scales):\n",
        "    super(ImagePyramide, self).__init__()\n",
        "    self.num_channels = 3\n",
        "    self.downs = {}\n",
        "    \n",
        "    for scale in scales:\n",
        "      self.downs[str(scale).replace('.', '-')] = AntiAliasInterpolation(self.num_channels, scale)\n",
        "\n",
        "  def call(self, x):\n",
        "    out_dict = {}\n",
        "    \n",
        "    for scale, down_module in self.downs.items():\n",
        "      out_dict['prediction_' + str(scale).replace('-', '.')] = down_module(x)\n",
        "\n",
        "    return out_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZxKSxK81j1V",
        "colab_type": "text"
      },
      "source": [
        "## Hourglass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaRN2Hqp20YT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DownBlock(tf.keras.Model):\n",
        "  def __init__(self, num_features):\n",
        "    super(DownBlock, self).__init__()\n",
        "    self.padding = [[0, 0], [1, 1], [1, 1], [0, 0]]\n",
        "    self.conv = layers.Conv2D(num_features, (3, 3), strides=1, padding=self.padding, use_bias=False)\n",
        "    self.batch_norm = layers.BatchNormalization()\n",
        "  \n",
        "  def call(self, input_layer):\n",
        "    block = self.conv(input_layer)\n",
        "    block = self.batch_norm(block)\n",
        "    block = layers.ReLU()(block)\n",
        "    block = layers.AveragePooling2D()(block)\n",
        "\n",
        "    return block"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWUwD0ThAj6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UpBlock(tf.keras.Model):\n",
        "  def __init__(self, num_features):\n",
        "    super(UpBlock, self).__init__()\n",
        "    self.padding = [[0, 0], [1, 1], [1, 1], [0, 0]]\n",
        "    self.conv = layers.Conv2D(num_features, (3, 3), strides=1, padding=self.padding, use_bias=False)\n",
        "    self.batch_norm = layers.BatchNormalization()\n",
        "  \n",
        "  def call(self, input_layer):\n",
        "    block = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(input_layer)\n",
        "    block = self.conv(block)\n",
        "    block = self.batch_norm(block)\n",
        "    block = layers.ReLU()(block)\n",
        "\n",
        "    return block"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaQlnGia1lx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Hourglass(tf.keras.Model):\n",
        "  def __init__(self, down_features_list, up_features_list, num_blocks=5):\n",
        "    super(Hourglass, self).__init__()\n",
        "    encoder_blocks = []\n",
        "\n",
        "    for i in range(num_blocks):\n",
        "      num_features = down_features_list[i]\n",
        "      encoder_blocks.append(DownBlock(num_features))\n",
        "    \n",
        "    self.encoder_blocks = encoder_blocks\n",
        "\n",
        "    decoder_blocks = []\n",
        "\n",
        "    for i in range(num_blocks):\n",
        "      num_features = up_features_list[i]\n",
        "      decoder_blocks.append(UpBlock(num_features))\n",
        "    \n",
        "    self.decoder_blocks = decoder_blocks\n",
        "  \n",
        "  def call(self, x):\n",
        "    down_block_list = [x]\n",
        "    for down_block in self.encoder_blocks:\n",
        "      down_block_list.append(down_block(down_block_list[-1]))\n",
        "    \n",
        "    model = down_block_list.pop()\n",
        "\n",
        "    for up_block in self.decoder_blocks:\n",
        "      model = up_block(model)\n",
        "      skip = down_block_list.pop()\n",
        "      model = tf.concat([model, skip], axis=-1)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX4dE0URILvJ",
        "colab_type": "text"
      },
      "source": [
        "## Coordinate grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-YHIRwMIPUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_coordinate_grid(spatial_size, type):\n",
        "  height, width = spatial_size\n",
        "  x = tf.keras.backend.arange(width, dtype=type)\n",
        "  y = tf.keras.backend.arange(height, dtype=type)\n",
        "\n",
        "  x = (2 * (x / (width - 1)) - 1)\n",
        "  y = (2 * (y / (height - 1)) - 1)\n",
        "  \n",
        "  yy = tf.tile(tf.reshape(y, (-1, 1)), [1, width])\n",
        "  xx = tf.tile(tf.reshape(x, (1, -1)), [height, 1])\n",
        "\n",
        "  meshed = tf.concat([tf.expand_dims(xx, axis=2), tf.expand_dims(yy, axis=2)], axis=2)\n",
        "  # shape 256, 256, 2\n",
        "\n",
        "  return meshed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vdaO9Y4qXTP",
        "colab_type": "text"
      },
      "source": [
        "## keypoints to gaussian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO1NTrpLpzX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def keypoints_to_gaussian(keypoints, spatial_size, kp_variance):\n",
        "  # TD<-R or TS<-R in equation (6)\n",
        "  mean = keypoints[\"value\"]\n",
        "  # shape batch, 10, 2\n",
        "\n",
        "  # Z in equation (6)\n",
        "  coordinate_grid = make_coordinate_grid(spatial_size, mean.dtype)\n",
        "  # shape height x width x 2\n",
        "  \n",
        "  coordinate_grid = tf.expand_dims(tf.expand_dims(coordinate_grid, axis=0), axis=0)\n",
        "  # 1 x 1 x height x width x 2\n",
        "\n",
        "  repeats = mean.shape[:2] + (1, 1, 1)\n",
        "  # batch x 10 x 1 x 1 x 1\n",
        "  coordinate_grid = tf.tile(coordinate_grid, multiples=repeats)\n",
        "  # batch x 10 x height x width x 2\n",
        "\n",
        "  # Preprocess kp shape\n",
        "  shape = mean.shape[:2] + (1, 1, 2)\n",
        "  # batch x 10 x 1 x 1 x 2\n",
        "\n",
        "  mean = tf.reshape(mean, shape)\n",
        "  # batch x 10 x 1 x 1 x 2\n",
        "\n",
        "  mean_sub = (mean - coordinate_grid)\n",
        "  # batch x 10 x 1 x 1 x 2 -\n",
        "  # batch x 10 x height x width x 2\n",
        "\n",
        "  out = tf.exp(-0.5 * tf.keras.backend.sum(mean_sub ** 2, axis=-1) / kp_variance)\n",
        "  # batch x 10 x height x width\n",
        "\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWIEmkHkjLyY",
        "colab_type": "text"
      },
      "source": [
        "## VGG19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQqEftYojdNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vgg19(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    layers = ['block1_conv2', 'block2_conv2', 'block3_conv2', 'block4_conv2', 'block5_conv2'] \n",
        "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "    vgg.trainable = False\n",
        "    \n",
        "    outputs = [vgg.get_layer(name).output for name in layers]\n",
        "\n",
        "    self.model = tf.keras.Model([vgg.input], outputs)\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = tf.keras.applications.vgg19.preprocess_input(x)\n",
        "    return model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTy4r-I0yQaY",
        "colab_type": "text"
      },
      "source": [
        "# Keypoint Detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K4Y1wWXyR_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# height, width, channels = 3\n",
        "class KeypointDetector(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(KeypointDetector, self).__init__()\n",
        "    self.scale_factor = 0.25\n",
        "    self.num_jacobian_maps = 10\n",
        "    self.num_keypoints = 10\n",
        "    self.num_channels = 3\n",
        "    self.down_features_list = [64, 128, 256, 512, 1024]\n",
        "    self.up_features_list = [512, 256, 128, 64, 32]\n",
        "    self.num_blocks = 5\n",
        "    self.temperature = 0.1\n",
        "\n",
        "    self.predictor = Hourglass(self.down_features_list, self.up_features_list, self.num_blocks) # Outputs height x width x 35\n",
        "    self.keypoints_map = layers.Conv2D(self.num_keypoints, (7, 7), strides=1, padding='valid')\n",
        "\n",
        "    # Initialize the weights/bias with identity transformation localisation network\n",
        "    weigth_initializer = tf.keras.initializers.zeros()\n",
        "    bias_initializer = tf.keras.initializers.constant([1, 0, 0, 1] * 10)\n",
        "    self.jacobian = layers.Conv2D(self.num_keypoints * 4, (7, 7), strides=1, padding='valid', bias_initializer=bias_initializer, kernel_initializer=weigth_initializer)\n",
        "\n",
        "    self.down = AntiAliasInterpolation(self.num_channels, self.scale_factor)\n",
        "  \n",
        "  def get_gaussian_keypoints(self, heatmap):\n",
        "    # heatmaps are confidence maps\n",
        "    # compute soft-argmax (get the coords of the maximum values of the heatmap) differentiable\n",
        "    heatmap = tf.expand_dims(heatmap, -1)\n",
        "    # shape batch x 250 x 250 x 10 x 1\n",
        "    grid = make_coordinate_grid(heatmap.shape[1:3], heatmap.dtype)\n",
        "    # shape 250 x 250 x 2\n",
        "    grid = tf.expand_dims(grid, axis=2)\n",
        "    # shape 250 x 250 x 1 x 2\n",
        "    grid = tf.expand_dims(grid, axis=0)\n",
        "    # shape 1 x 250 x 250 x 1 x 2\n",
        "\n",
        "    value = heatmap * grid\n",
        "    # shape batch x 250 x 250 x 10 x 2\n",
        "    value = tf.keras.backend.sum(value, axis=[1, 2])\n",
        "    # shape batch x 10 x 2\n",
        "\n",
        "    # keypoints are in a range [-1, 1] due to the grid\n",
        "    kp = {'value': value}\n",
        "\n",
        "    return kp\n",
        "  \n",
        "  def call(self, x):\n",
        "    model = self.down(x)\n",
        "    feature_map = self.predictor(model)\n",
        "    raw_keypoints = self.keypoints_map(feature_map)\n",
        "\n",
        "    final_shape = raw_keypoints.shape # pytorch 4, 10, 5, 5 tf: 4, 5, 5, 10\n",
        "\n",
        "    heatmap = tf.keras.activations.softmax(raw_keypoints / self.temperature, axis=[1, 2])\n",
        "    # temperature increase the values so is easier to compute the soft-argmax\n",
        "    final_keypoints = self.get_gaussian_keypoints(heatmap)\n",
        "\n",
        "    jacobian_map = self.jacobian(feature_map)\n",
        "    # batch x height x width x 40\n",
        "\n",
        "    jacobian_map = tf.reshape(jacobian_map, [final_shape[0], final_shape[1], final_shape[2], self.num_jacobian_maps, 4])\n",
        "    # batch x height x width x 10 x 4\n",
        "\n",
        "    heatmap = tf.expan_dims(heatmap, axis=-1)\n",
        "    # batch x height x width x 10 x 1\n",
        "\n",
        "    jacobian = heatmap * jacobian_map # reduce the importance of the places far from the keypoints coords\n",
        "    # batch x height x width x 10 x 4\n",
        "\n",
        "    jacobian = tf.reshape(jacobian, [final_shape[0], -1, final_shape[3], 4])\n",
        "    # batch x (height * width) x 10 x 4\n",
        "\n",
        "    jacobian = tf.keras.backend.sum(jacobian, axis=1)\n",
        "    # batch x 10 x 4\n",
        "\n",
        "    jacobian = tf.reshape(jacobian, [jacobian.shape[0], jacobian.shape[1], 2, 2])\n",
        "    # batch x 10 x 2 x 2\n",
        "    # shape batch, 10, 2, 2 where 10: keypoints each with a jacobian of size 2x2\n",
        "\n",
        "    final_keypoints['jacobian'] = jacobian\n",
        "\n",
        "    return final_keypoints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCnkTEGXM7bh",
        "colab_type": "text"
      },
      "source": [
        "# Dense motion network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb-PYDelM9Hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DenseMotionNetwork(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(DenseMotionNetwork, self).__init__()\n",
        "    # input shape height x width x 44\n",
        "    self.num_blocks = 5\n",
        "    self.num_channels = 3\n",
        "    self.num_keypoints = 10\n",
        "    self.scale_factor = 0.25\n",
        "    self.kp_variance = 0.01\n",
        "    self.down_features_list = [128, 256, 512, 1024, 1024]\n",
        "    self.up_features_list = [1024, 512, 256, 128, 64]\n",
        "    self.padding = [[0, 0], [3, 3], [3, 3], [0, 0]] # pad only height, width\n",
        "\n",
        "    self.hourglass = Hourglass(self.down_features_list, self.up_features_list, self.num_blocks) # Outputs height x width x 67\n",
        "    self.mask = layers.Conv2D(self.num_keypoints + 1, (7, 7), strides=1, padding=\"valid\")\n",
        "    self.occlusion = layers.Conv2D(1, (7, 7), strides=1, padding=\"valid\")\n",
        "    self.down = AntiAliasInterpolation(self.num_channels, self.scale_factor)\n",
        "  \n",
        "  def create_heatmap_representations(self, image_size, kp_driving, kp_source):\n",
        "    spatial_size = image_size[1:4]\n",
        "    # shape 256 x 256\n",
        "\n",
        "    gaussian_driving = keypoints_to_gaussian(kp_driving, spatial_size=spatial_size, kp_variance=self.kp_variance)\n",
        "    gaussian_source = keypoints_to_gaussian(kp_source, spatial_size=spatial_size, kp_variance=self.kp_variance)\n",
        "    # shape batch, 10, 256, 256\n",
        "\n",
        "    heatmap = gaussian_driving - gaussian_source\n",
        "    # batch x 10 x 256 x 256\n",
        "\n",
        "    zeros = tf.zeros((heatmap.shape[0], 1, spatial_size[0], spatial_size[1]), dtype=heatmap.dtype)\n",
        "    # shape batch x 1 x 256 x 256\n",
        "\n",
        "    heatmap = tf.concat([zeros, heatmap], axis=1)\n",
        "    # shape batch x 11 x 256 x 256\n",
        "\n",
        "    heatmap = tf.expand_dims(heatmap, axis=-1)\n",
        "    # shape batch x 11 x 256 x 256 x 1\n",
        "\n",
        "    return heatmap\n",
        "  \n",
        "  def create_sparse_motions(self, image_size, kp_driving, kp_source):\n",
        "    batch_size, height, width, _ = image_size\n",
        "    # Z in equation (4)\n",
        "    identity_grid = make_coordinate_grid((height, width), type=kp_source['value'].dtype)\n",
        "    # shape 256 x 256 x 2\n",
        "\n",
        "    identity_grid = tf.expand_dims(tf.expand_dims(identity_grid, axis=0), axis=0)\n",
        "    # shape 1 x 1 x 256 x 256 x 2\n",
        "\n",
        "    # TD<-R in equation (4)\n",
        "    driving_keypoints = kp_driving['value']\n",
        "    # shape batch x 10 x 2\n",
        "    shape = driving_keypoints.shape[:2] + (1, 1, 2)\n",
        "    driving_keypoints = tf.reshape(driving_keypoints, shape)\n",
        "    # shape batch, 10, 1, 1, 2\n",
        "\n",
        "    # Z - TD<-R in equation (4)\n",
        "    coordinate_grid = identity_grid - driving_keypoints\n",
        "    # shape batch, 10, 256, 256, 2\n",
        "\n",
        "    # Using the inverse of d/dp Td <- R ; Equation (5) Jk\n",
        "    jacobian = tf.linalg.matmul(kp_source['jacobian'], tf.linalg.inv(kp_driving['jacobian']))\n",
        "    # shape batch x 10 x 2 x 2\n",
        "\n",
        "    jacobian = tf.expand_dims(tf.expand_dims(jacobian, axis=-3), axis=-3)\n",
        "    # shape batch x 10 x 1 x 1 x 2 x 2\n",
        "\n",
        "    jacobian = tf.tile(jacobian, [1, 1, height, width, 1, 1])\n",
        "    # shape batch x 10 x 256 x 256 x 2 x 2\n",
        "\n",
        "    # Jk . (Z - TD<-R) in equation (4)\n",
        "    coordinate_grid = tf.linalg.matmul(jacobian, tf.expand_dims(coordinate_grid, axis=-1))\n",
        "    # shape batch x 10 x 256 x 256 x 2 x 1\n",
        "\n",
        "    coordinate_grid = tf.squeeze(coordinate_grid) # remove last axis\n",
        "    # shape batch x 10 x 256 x 256 x 2\n",
        "\n",
        "    source_keypoints = kp_source['value']\n",
        "    # shape batch x 10 x 2    \n",
        "\n",
        "    shape = source_keypoints.shape[:2] + (1, 1, 2)\n",
        "    source_keypoints = tf.reshape(source_keypoints, shape)\n",
        "    # shape batch x 10 x 1 x 1 x 2\n",
        "\n",
        "    # Ts <- D(z) where source_keypoints is TS<-R and coordinate_grid is Jk . (Z - TD<-R)\n",
        "    driving_to_source = source_keypoints + coordinate_grid \n",
        "    # shape batch x 10 x 256 x 256 x 2\n",
        "               \n",
        "    # Adding background feature, background feature is just the identity_grid without motions\n",
        "    identity_grid = tf.tile(identity_grid, [batch_size, 1, 1, 1, 1])\n",
        "    # shape batch x 1 x 256 x 256 x 2\n",
        "\n",
        "    sparse_motions = tf.concat([identity_grid, driving_to_source], axis=1)\n",
        "    # shape batch x 11 x 256 x 256 x 2 \n",
        "    # 11 channels since we estimate the taylor aproximation for each keypoint\n",
        "\n",
        "    return sparse_motions\n",
        "\n",
        "  def create_deformed_source_image(self, source_image, sparse_motions):\n",
        "    batch_size, _, height, width = source_image.shape\n",
        "    # batch x 256 x 256 x 3\n",
        "\n",
        "    source_repeat = tf.expand_dims(tf.expand_dims(source_image, axis=1))\n",
        "    # batch x 1 x 256 x 256 x 3\n",
        "    \n",
        "    source_repeat = tf.tile(source_repeat, [1, self.num_keypoints + 1, 1, 1, 1])\n",
        "    # batch x 11 x 256 x 256 x 3\n",
        "\n",
        "    source_repeat = tf.reshape(source_repeat, [batch_size * (self.num_keypoints + 1), height, width, -1])\n",
        "    # (batch . 11) x 256 x 256 x 3\n",
        "    \n",
        "    sparse_motions = tf.reshape(sparse_motions, [batch_size * (self.num_keypoints + 1), height, width, -1])\n",
        "    # (batch . 11) x 256 x 256 x 2\n",
        "\n",
        "    new_max = width - 1\n",
        "    new_min = 0\n",
        "    sparse_motions = (new_max - new_min) / (tf.keras.backend.max(sparse_motions) - tf.keras.backend.min(sparse_motions)) * (sparse_motions - tf.keras.backend.max(sparse_motions)) + new_max\n",
        "\n",
        "    sparse_deformed = tfa.image.resampler(source_repeat, sparse_motions)\n",
        "    # (batch . 11) x 256 x 256 x 3\n",
        "\n",
        "    sparse_deformed = tf.reshape(sparse_deformed, [batch_size, (self.num_keypoints + 1), height, width, -1])\n",
        "    # batch x 11 x 256 x 256 x 3\n",
        "\n",
        "    return sparse_deformed\n",
        "  \n",
        "  def call(self, source_image, kp_driving, kp_source):\n",
        "    source_image = self.down(source_image)\n",
        "    image_size = source_image.shape\n",
        "    batch_size, height, width, _ = image_size\n",
        "    out_dict = dict()\n",
        "\n",
        "    heatmap_representation = self.create_heatmap_representations(image_size, kp_driving, kp_source)\n",
        "    # shape batch x 11 x 256 x 256 x 1\n",
        "    sparse_motion = self.create_sparse_motions(image_size, kp_driving, kp_source) # keypoint k of d to s\n",
        "    # shape batch x 11 x 256 x 256 x 2\n",
        "    warped_images = self.create_deformed_source_image(source_image, sparse_motion)\n",
        "    # shape batch x 11 x 256 x 256 x 3\n",
        "\n",
        "    # Debug/print\n",
        "    out_dict['warped_images'] = warped_images # sparse_deformed\n",
        "\n",
        "    input = tf.concat([heatmap_representation, warped_images], axis=-1)\n",
        "    # shape batch x 22 x 256 x 256 x 3\n",
        "\n",
        "    input = tf.permute(input, [0, 2, 3, 1, 4])\n",
        "    # shape batch x 256 x 256 x 22 x 3\n",
        "    \n",
        "    input = tf.reshape(input, [batch_size, height, width, -1])\n",
        "    # shape batch x 256 x 256 x 66\n",
        "\n",
        "    prediction = self.hourglass(input)\n",
        "    # batch x height x width x 35\n",
        "\n",
        "    prediction = tf.pad(prediction, self.padding)\n",
        "\n",
        "    mask = self.mask(prediction)\n",
        "    # batch x height x width x 11\n",
        "\n",
        "    mask = tf.keras.activations.softmax(mask)\n",
        "    # Along the last axis. Thus, we don't repeat values along axes (each keypoint only appears in one channel)\n",
        "    # batch x height x width x 11\n",
        "\n",
        "    # Debug/print\n",
        "    out_dict['mask'] = mask\n",
        "\n",
        "    mask = tf.expand_dims(mask, axis=-1)\n",
        "    # batch x height x width x 11 x 1\n",
        "\n",
        "    sparse_motion = tf.transpose(sparse_motion, [0, 2, 3, 1, 4])\n",
        "    # batch x 256 x 256 x 11 x 2\n",
        "\n",
        "    deformation = (sparse_motion * mask)\n",
        "    deformation = tf.keras.backend.sum(sparse_motion, axis=3) \n",
        "    # batch x 256 x 256 x 2\n",
        "\n",
        "    out_dict['dense_optical_flow'] = deformation # deformation\n",
        "\n",
        "    occlusion_map = tf.keras.activations.sigmoid(self.occlusion(prediction))\n",
        "    # shape batch x 256 x 256 x 1\n",
        "\n",
        "    out_dict['occlusion_map'] = occlusion_map\n",
        "\n",
        "    return out_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAEsh_H5aBH1",
        "colab_type": "text"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyAdtCtaf8Q2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    block_expansion = 64\n",
        "    self.padding = [[0, 0], [3, 3], [3, 3], [0, 0]] # pad only height, width\n",
        "    self.num_channels = 3\n",
        "    self.num_keypoints = 10\n",
        "    self.num_blocks = 2\n",
        "    self.num_bottleneck_blocks = 6\n",
        "    \n",
        "    self.dense_motion_network = DenseMotionNetwork()\n",
        "    self.first = SameBlock2d(block_expansion)\n",
        "\n",
        "    self.down_features_list = [128, 256]\n",
        "    self.up_features_list = [128, 64] \n",
        "\n",
        "    encoder_blocks = []\n",
        "\n",
        "    for i in range(num_blocks):\n",
        "      num_features = self.down_features_list[i]\n",
        "      encoder_blocks.append(DownBlock(num_features))\n",
        "    \n",
        "    self.encoder_blocks = encoder_blocks\n",
        "\n",
        "    decoder_blocks = []\n",
        "\n",
        "    for i in range(num_blocks):\n",
        "      num_features = self.up_features_list[i]\n",
        "      decoder_blocks.append(UpBlock(num_features))\n",
        "    \n",
        "    self.decoder_blocks = decoder_blocks\n",
        "\n",
        "    self.bottleneck = tf.keras.Sequential()\n",
        "\n",
        "    for i in range(num_bottleneck_blocks):\n",
        "      self.bottleneck.add(ResBlock2d(self.down_features_list[-1]))\n",
        "\n",
        "    self.final = layers.Conv2D(self.num_channels, strides=1, padding=\"valid\")\n",
        "\n",
        "  def deform_input(self, x, deformation):\n",
        "    _, height_old, width_old, _ = deformation.shape\n",
        "    _ height, width, _ = x.shape\n",
        "\n",
        "    if height_old != height or width_old != width:\n",
        "      deformation = interpolate_tensor(deformation, width)\n",
        "\n",
        "    new_max = width - 1\n",
        "    new_min = 0\n",
        "    deformation = (new_max - new_min) / (tf.keras.backend.max(deformation) - tf.keras.backend.min(deformation)) * (deformation - tf.keras.backend.max(deformation)) + new_max\n",
        "\n",
        "    return tfa.image.resampler(x, deformation)\n",
        "  \n",
        "  def call(self, source_image, kp_driving, kp_source):\n",
        "    out = self.first(tf.pad(source_image, self.padding))\n",
        "\n",
        "    for down_block in self.encoder_blocks:\n",
        "      out = down_block(out)\n",
        "    \n",
        "    output_dict = {}\n",
        "\n",
        "    dense_motion = self.dense_motion_network(source_image, kp_driving, kp_source)\n",
        "\n",
        "    # Debug/print                                       \n",
        "    output_dict['mask'] = dense_motion['mask']\n",
        "    # Debug/print\n",
        "    output_dict['warped_images'] = dense_motion['warped_images'] # sparse_deformed\n",
        "\n",
        "    occlusion_map = dense_motion['occlusion_map']\n",
        "    # shape batch x 256 x 256 x 1\n",
        "    \n",
        "    # Debug/print\n",
        "    output_dict['occlusion_map'] = occlusion_map\n",
        "\n",
        "    dense_optical_flow = dense_motion['dense_optical_flow'] # deformation\n",
        "    # batch x 256 x 256 x 2 \n",
        "    out = self.deform_input(out, dense_optical_flow)\n",
        "    # batch x 256 x 256 x 2 \n",
        "\n",
        "    if out.shape[1] != occlusion_map.shape[1] or out.shape[2] != occlusion_map.shape[2]:\n",
        "      occlusion_map = interpolate_tensor(occlusion_map, out[1])\n",
        "    \n",
        "    out = out * occlusion_map\n",
        "\n",
        "    # Debug/print\n",
        "    output_dict[\"aligned_features\"] = self.deform_input(source_image, dense_optical_flow) # deformed\n",
        "    \n",
        "    # Decoder part\n",
        "\n",
        "    out = self.bottleneck(out)\n",
        "\n",
        "    for up_block in self.decoder_blocks:\n",
        "      out = up_block(out)\n",
        "\n",
        "    out = self.final(tf.pad(out, self.padding))\n",
        "    out = tf.keras.activations.sigmoid(out)\n",
        "\n",
        "    output_dict[\"prediction\"] = out\n",
        "\n",
        "    return output_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD9NtbBgOgdZ",
        "colab_type": "text"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jow2msKtOl23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.num_channels = 3\n",
        "    self.num_keypoints = 10\n",
        "    self.num_blocks = 4\n",
        "    self.num_features_list = [64, 128, 256, 512] \n",
        "    self.kp_variance = 0.01\n",
        "\n",
        "    encoder_blocks = []\n",
        "\n",
        "    for i in range(num_blocks):\n",
        "      num_features = self.num_features_list[i]\n",
        "      norm = i != 0\n",
        "      pool = i != num_blocks - 1\n",
        "      encoder_blocks.append(DownBlock2d(num_features, norm, pool))\n",
        "    \n",
        "    self.encoder_blocks = encoder_blocks\n",
        "    self.conv = layers.Conv2D(1, kernel_size=1)\n",
        "\n",
        "  def call(self, x, key_points):\n",
        "    feature_maps = []\n",
        "    out = x\n",
        "    # batch x height x width x 3\n",
        "    heatmap = keypoints_to_gaussian(key_points, x.shape[1:3], self.kp_variance)\n",
        "    # batch x 10 x height x width\n",
        "    out = tf.concat([out, tf.permute(heatmap, [0, 2, 3, 1])], axis=-1)\n",
        "    # batch x height x width x 13\n",
        "\n",
        "    for block in self.encoder_blocks:\n",
        "      feature_maps.append(block(out))\n",
        "      out = feature_maps[-1]\n",
        "    \n",
        "    prediction_map = self.conv(out)\n",
        "    # batch x height x width x 1\n",
        "\n",
        "    return feature_maps, prediction_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO71ZAKwEcGM",
        "colab_type": "text"
      },
      "source": [
        "# Transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We6W82pPEapR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transform:\n",
        "  def __init__(self, batch_size):\n",
        "    self.sigma_affine = 0.05\n",
        "    self.sigma_tps = 0.005\n",
        "    self.points_tps = 5\n",
        "    noise = tf.random.normal((batch_size, 2, 3), mean=0, stddev=self.sigma_affine)\n",
        "    # eye returns an identity matrix\n",
        "    self.theta = noise + tf.expand_dims(tf.eye(2, 3), axis=0)\n",
        "    # shape batch x 2 x 3\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.control_points = make_coordinate_grid((self.points_tps, self.points_tps), type=noise.dtype)\n",
        "    self.control_points = tf.expand_dims(self.control_points, axis=0)\n",
        "    # shape 1 x 5 x 5 x 2\n",
        "    self.control_params = tf.random.normal((batch_size, 1, self.points_tps ** 2), mean=0, stddev=self.sigma_tps)\n",
        "    # shape batch x 1 x 25\n",
        "  \n",
        "  def transform_frame(self, frame):\n",
        "    grid = make_coordinate_grid(frame.shape[1:3], type=frame.dtype)\n",
        "    grid = tf.expand_dims(grid, axis=0)\n",
        "    grid = tf.reshape(grid, [1, frame.shape[1] * frame.shape[2], 2])\n",
        "    # shape 1 x (height * width) x 2\n",
        "    grid = self.warp_coordinates(grid)\n",
        "    # batch x new_size x 2\n",
        "    grid = tf.reshape(grid, [self.batch_size, frame.shape[1], frame.shape[2], 2])\n",
        "    # batch x 256 x 256 x 2\n",
        "    \n",
        "    new_max = frame.shape[2] - 1\n",
        "    new_min = 0\n",
        "    grid = (new_max - new_min) / (tf.keras.backend.max(grid) - tf.keras.backend.min(grid)) * (grid - tf.keras.backend.max(grid)) + new_max\n",
        "\n",
        "    return tfa.image.resampler(frame, grid)\n",
        "    # return F.grid_sample(frame, grid, padding_mode=\"reflection\")\n",
        "  \n",
        "  def warp_coordinates(self, coordinates):\n",
        "    theta = tf.cast(self.theta, coordinates.dtype)\n",
        "    theta = tf.expand_dims(theta, axis=1)\n",
        "    # shape batch x 1 x 2 x 3\n",
        "\n",
        "    # coordinates shape can be \n",
        "    #     1 x (height * width) x 2\n",
        "    # batch x num_keypoints x 2\n",
        "    transformed = tf.linalg.matmul(theta[:, :, :, :2], tf.expand_dims(coordinates, axis=-1)) + theta[:, :, :, 2:]\n",
        "    # shape batch x (height * width) x 2 x 1 or\n",
        "    # shape batch x num_keypoints x 2 x 1\n",
        "    transformed = tf.squeeze(transformed, axis=-1)\n",
        "    # shape batch x (height * width) x 2\n",
        "\n",
        "    control_points = tf.cast(self.control_points, coordinates.dtype)\n",
        "    # shape 1 x 5 x 5 x 2\n",
        "    control_params = tf.cast(self.control_params, coordinates.dtype)\n",
        "    # shape batch x 1 x 25\n",
        "\n",
        "    # coordinates = xi, yi,  control_points = x, y\n",
        "    distances = tf.reshape(coordinates, [coordinates.shape[0], -1, 1, 2]) - tf.reshape(control_points, [1, 1, -1, 2])\n",
        "    # shape 1 x new_size x 25 x 2\n",
        "\n",
        "    distances = tf.keras.backend.sum(tf.abs(distances), axis=-1)\n",
        "    # shape 1 x new_size x 25\n",
        "\n",
        "    result = distances ** 2\n",
        "\n",
        "    result = result * tf.math.log(distances + 1e-6)\n",
        "    result = result * control_params\n",
        "    # batch x new_size x 25\n",
        "\n",
        "    result = tf.keras.backend.sum(result, axis=2)\n",
        "    # batch x new_size\n",
        "    result = tf.reshape(result, [self.batch_size, coordinates.shape[1], 1])\n",
        "    # batch x new_size x 1\n",
        "    transformed = transformed + result\n",
        "    # batch x new_size x 2\n",
        "\n",
        "    return transformed\n",
        "\n",
        "  def jacobian(self, coordinates):\n",
        "    new_coordinates = self.warp_coordinates(coordinates)\n",
        "    x = tf.keras.backend.sum(new_coordinates[..., 0])\n",
        "    y = tf.keras.backend.sum(new_coordinates[..., 1])\n",
        "\n",
        "    grad_x = tape.gradient(x, coordinates) \n",
        "    grad_y = tape.gradient(y, coordinates)\n",
        "\n",
        "    return tf.concat([tf.expand_dims(grad_x, axis=-2), tf.expand_dims(grad_y, axis=-2)], axis=-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmGbXzFIoPfV",
        "colab_type": "text"
      },
      "source": [
        "# Full Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSWjtWHQnrO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FullGenerator(tf.keras.Model):\n",
        "  def __init__(self, key_point_detector, generator, discriminator):\n",
        "    super(FullGenerator, self).__init__()\n",
        "    self.feature_matching_weights = 10\n",
        "    self.equivariance_weights = 10\n",
        "    self.perceptual_weights = [10, 10, 10, 10, 10]\n",
        "    self.scales = [1, 0.5, 0.25, 0.125]\n",
        "    self.vgg = Vgg19()\n",
        "    self.pyramid = ImagePyramide(self.scales)\n",
        "    self.key_point_detector = key_point_detector\n",
        "    self.generator = generator\n",
        "    self.discriminator = discriminator\n",
        "\n",
        "  def call(self, source_images, driving_images):\n",
        "    kp_source = self.key_point_detector(source_images)\n",
        "    kp_driving = self.key_point_detector(driving_images)\n",
        "\n",
        "    generated = self.generator(source_images, kp_source=kp_source, kp_driving=kp_driving)\n",
        "    # Debug/print\n",
        "    generated.update({'kp_source': kp_source, 'kp_driving': kp_driving})\n",
        "\n",
        "    loss_values = {}\n",
        "\n",
        "    pyramide_real = self.pyramid(driving_images)\n",
        "    pyramide_generated = self.pyramid(generated['prediction'])\n",
        "\n",
        "    # Perceptual loss (Loss for gan generator)\n",
        "    perceptual_loss = 0\n",
        "    for scale in self.scales:\n",
        "      x_vgg = self.vgg(pyramide_generated['prediction_' + str(scale)])\n",
        "      y_vgg = self.vgg(pyramide_real['prediction_' + str(scale)])\n",
        "\n",
        "      for i, weight in enumerate(self.perceptual_weights):\n",
        "        loss = tf.reduce_mean(tf.abs(x_vgg[i] - tf.stop_gradient(y_vgg[i])))\n",
        "        perceptual_loss += self.perceptual_weights[i] * loss\n",
        "      loss_values['perceptual'] = perceptual_loss\n",
        "    \n",
        "    # Gan loss (only one scale used, the original [1])\n",
        "\n",
        "    # We detach the keypoints here so we dont compue its gradients and we use it as input images!!!\n",
        "    discriminator_maps_real, _ = self.discriminator(driving_images, kp=detach_keypoint(kp_driving))\n",
        "    discriminator_maps_generated, discriminator_pred_map_generated = self.discriminator(generated['prediction'], kp=detach_keypoint(kp_driving))\n",
        "    \n",
        "    # LSGAN G Loss\n",
        "    # Discriminator outputs a pathmap like pix2pix where 1 labels are for real images and 0 labels are for generated images\n",
        "    # Since we want to fool the discriminator we want our generated images to output 1\n",
        "    gan_loss = tf.reduce_mean((discriminator_pred_map_generated - 1) ** 2)\n",
        "    # same as tf.reduce_mean(tf.keras.losses.mean_squared_error(tf.ones_like(discriminator_pred_map_generated), discriminator_pred_map_generated))\n",
        "    gan_loss += self.loss_weights['generator_gan'] * gan_loss\n",
        "    loss_values['gen_gan'] = gan_loss\n",
        "    \n",
        "    # feature_matching loss\n",
        "    feature_matching_loss = tf.reduce_mean(tf.abs(discriminator_maps_real - discriminator_maps_generated))\n",
        "    feature_matching_loss += self.feature_matching_weights * feature_matching_loss\n",
        "\n",
        "    loss_values['feature_matching'] = feature_matching_loss\n",
        "\n",
        "    # Equivariance Loss\n",
        "    batch_size = driving_images.shape[0]\n",
        "    transform = Transform(batch_size)\n",
        "\n",
        "    transformed_frame = transform.transform_frame(driving_images)\n",
        "    # image Y\n",
        "    # shape batch x height x width x 2\n",
        "\n",
        "    transformed_keypoints = self.key_point_detector(transformed_frame)\n",
        "    # Ty <-R\n",
        "\n",
        "    # Debug/print\n",
        "    generated['transformed_frame'] = transformed_frame\n",
        "    # Debug/print\n",
        "    generated['transformed_kp'] = transformed_keypoints\n",
        "\n",
        "    keypoints_loss = tf.reduce_mean(tf.abs(kp_driving['value'] - transform.warp_coordinates(transformed_keypoints['value'])))\n",
        "    loss_values['equivariance_value'] = self.equivariance_weights * keypoints_loss\n",
        "\n",
        "    # Here we apply the transformation for a second time and then compute the jacobian\n",
        "    jacobian_transformed = tf.linalg.matmul(transform.jacobian(transformed_keypoints['value']), transformed_keypoints['jacobian'])\n",
        "    # Equivariance properties\n",
        "\n",
        "    normed_driving = tf.linalg.inv(kp_driving['jacobian']) #inverse of Tx <-R\n",
        "    normed_transformed = jacobian_transformed\n",
        "\n",
        "    jacobian_mul = tf.linalg.matmul(normed_driving, normed_transformed)\n",
        "    identity_matrix = tf.cast(tf.reshape(tf.eye(2), [1, 1, 2, 2]), jacobian_mul.dtype)\n",
        "    jacobian_loss = tf.reduce_mean(tf.abs(identity_matrix - jacobian_mul))\n",
        "    loss_values['equivariance_jacobian'] = self.equivariance_weights * jacobian_loss\n",
        "\n",
        "    return loss_values, generated    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPL_vdUDP9uj",
        "colab_type": "text"
      },
      "source": [
        "# Full Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOy51JZvP_oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FullDiscriminator(tf.keras.Model):\n",
        "  def __init__(self, discriminator):\n",
        "    super(FullDiscriminator, self).__init__()\n",
        "    self.discriminator = discriminator\n",
        "\n",
        "  def call(self, x_driving, generated):\n",
        "    kp_driving = generated['kp_driving']\n",
        "\n",
        "    loss_values = {}\n",
        "\n",
        "    _, discriminator_pred_map_real = self.discriminator(x_driving, kp=detach_keypoint(kp_driving))\n",
        "    _, discriminator_pred_map_generated = self.discriminator(tf.stop_gradient(generated['prediction']), kp=detach_keypoint(kp_driving))\n",
        "    \n",
        "    # LSGAN\n",
        "    discriminator_loss = (1 - discriminator_pred_map_real) ** 2 + discriminator_pred_map_generated ** 2\n",
        "    # Where discriminator_pred_map_real should output 1's and discriminator_pred_map_generated 0's\n",
        "    loss_values['disc_gan'] = tf.reduce_mean(discriminator_loss)\n",
        "\n",
        "    return loss_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u3trc5ODSMn",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04wFbr9XDTvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 2e-4\n",
        "\n",
        "optimizer_generator = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.5, beta_2=0.999)\n",
        "optimizer_keypoint_detector = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.5, beta_2=0.999)\n",
        "optimizer_discriminator = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.5, beta_2=0.999)\n",
        "\n",
        "batch_size = 20\n",
        "epochs = 150"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCZ0qcMHK0vj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_full = GeneratorFullModel(kp_detector, generator, discriminator, train_params)\n",
        "discriminator_full = DiscriminatorFullModel(kp_detector, generator, discriminator, train_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiQ-GHQdDT-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(source_images, driving_images):\n",
        "  with tf.GradientTape(persistent=True) as tape: \n",
        "    losses_generator, generated = generator_full(source_images, driving_images)\n",
        "    generator_loss = tf.math.reduce_sum(list(losses_generator.values()))\n",
        "\n",
        "  generator_gradients = tape.gradient(generator_loss, generator_full.trainable_variables)\n",
        "  keypoint_detector_gradients = tape.gradient(generator_loss, keypoint_detector.trainable_variables)\n",
        "\n",
        "  optimizer_generator.apply_gradients(zip(generator_gradients, generator_full.trainable_variables))\n",
        "  optimizer_keypoint_detector.apply_gradients(zip(keypoint_detector_gradients, keypoint_detector.trainable_variables))\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    losses_discriminator = discriminator_full(x)\n",
        "    discriminator_loss = tf.math.reduce_sum(list(losses_discriminator.values()))\n",
        "  \n",
        "  discriminator_gradients = tape.gradient(discriminator_loss, discriminator_full.trainable_variables)\n",
        "  optimizer_discriminator.apply_gradients(zip(discriminator_gradients, discriminator_full.trainable_variables))\n",
        "\n",
        "  return generator_loss + discriminator_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9Om9JSKP7lO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decay_lr(optimizer, epoch):\n",
        "  if epoch >= 60 and epoch <= 90:\n",
        "    current_lr = tf.keras.backend.get_value(optimizer.lr)\n",
        "    new_lr = current_lr * 0.1\n",
        "    tf.keras.backend.set_value(optimizer.lr, new_lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02bcHvvaOOpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs, total_steps):\n",
        "  for epoch in range(epochs):\n",
        "    batch_time = time.time()\n",
        "    epoch_time = time.time()\n",
        "    step = 0\n",
        "\n",
        "    epoch_count = f\"0{epoch + 1}/{epochs}\" if epoch < 9 else f\"{epoch + 1}/{epochs}\"\n",
        "\n",
        "    for source_images, driving_images in zip(images_batches, labels_batches, masks_batches):\n",
        "      total_loss = train_step(source_images, driving_images)\n",
        "\n",
        "      loss = float(loss.numpy())\n",
        "      step += 1\n",
        "\n",
        "      print('\\r', 'Epoch', epoch_count, '| Step', f\"{step}/{train_steps}\",\n",
        "            '| loss:', f\"{loss:.5f}\", \"| Step time:\", f\"{time.time() - batch_time:.2f}\", end='')    \n",
        "      \n",
        "      batch_time = time.time()\n",
        "      total_steps += 1\n",
        "\n",
        "    loss_results.append(loss)\n",
        "    decay_lr(optimizer_generator, epoch)\n",
        "    decay_lr(optimizer_keypoint_detector, epoch)\n",
        "    decay_lr(optimizer_discriminator, epoch)\n",
        "\n",
        "    print('\\r', 'Epoch', epoch_count, '| Step', f\"{step}/{train_steps}\",\n",
        "          '| loss:', \"| Epoch time:\", f\"{time.time() - epoch_time:.2f}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-eMVlXJemd0",
        "colab_type": "text"
      },
      "source": [
        "# Interpolation test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9LJhQc3qsCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def interpolate(xs, ys, A, B):\n",
        "  epsilon = 0.0000001\n",
        "\n",
        "  # A second row\n",
        "  A_1, A_2 = A\n",
        "  # B first row\n",
        "  B_1, B_2 = B\n",
        "\n",
        "  # Always take both numbers into account but only choose x near to one or another pixel if x or y doesn't move\n",
        "\n",
        "  # Left to right coordinates\n",
        "  x1, x2, x = xs\n",
        "\n",
        "  weight_1 = (x - x1)/((x2 - x1) + epsilon)\n",
        "  print(\"weight_1\", weight_1)\n",
        "\n",
        "  R1 = A_1 + weight_1 * (A_2 - A_1) # Second row\n",
        "  R2 = B_1 + weight_1 * (B_2 - B_1) # First row\n",
        "\n",
        "  print(\"R1\", R1)\n",
        "  print(\"R2\", R2)\n",
        "\n",
        "  # Bottom to top coordinates\n",
        "  y1, y2, y = ys\n",
        "\n",
        "  weight_2 = (y - y1)/((y2 - y1) + epsilon)\n",
        "  print(\"weight_2\", weight_2)\n",
        "\n",
        "  P = R2 + weight_2 * (R1 - R2)\n",
        "\n",
        "  print(\"P\", P)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NG8sfnoFS2O",
        "colab_type": "text"
      },
      "source": [
        "### Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lc6BleqLm3l",
        "colab_type": "code",
        "outputId": "10bc4282-6eec-4504-9a13-439a7703f041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x1 = -0.2\n",
        "x2 = 0.2\n",
        "x = -0.14285714\n",
        "\n",
        "weight_1 = (x - x1)/(x2 - x1)\n",
        "\n",
        "A_1 = 45\n",
        "A_2 = 212\n",
        "\n",
        "R1 = A_1 + weight_1 * (A_2 - A_1)\n",
        "\n",
        "B_1 = 220\n",
        "B_2 = 199\n",
        "\n",
        "R2 = B_1 + weight_1 * (B_2 - B_1)\n",
        "print(R2, \"R2\")\n",
        "\n",
        "y2 = 0.6\n",
        "y1 = 1\n",
        "y = 1\n",
        "\n",
        "weight_2 = (y2 - y)/(y2 - y1)\n",
        "\n",
        "P = R2 + weight_2 * (R1 - R2)\n",
        "print(P)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "216.99999985 R2\n",
            "68.85714404999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W9TqeCZLLQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[133.0000, 166.5714, 125.5714,  47.2857,  18.7143,  57.0000,  96.5714, 113.0000]\n",
        "[ 77.2857,  93.0000,  93.2245,  83.6122,  75.9592,  55.9796,  59.9388, 104.4286]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIcJ9EqWRfyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[ 43.4286,  14.1429,  74.2449, 174.6735, 198.0408, 207.6939, 217.9592, 234.2857],\n",
        "[ 67.0000,  19.8571,  19.8571,  68.8571, 188.1429, 228.0000, 241.4286, 245.0000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSZcg1OS4kfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numpy_tensor = np.array([[133,  180, 53,  13, 90, 113],\n",
        "[55,  67, 98,  99, 23, 101],\n",
        "[34,  54, 89,  4, 12, 5],\n",
        "[56,  2, 3,  112, 45, 156],\n",
        "[34,  3, 220,  199, 200, 230],\n",
        "[67,  1, 45,  212, 240, 245]], dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6h8TQT1Jiam",
        "colab_type": "code",
        "outputId": "07d58591-8e31-4f12-d751-4db5012dd710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "numpy_tensor.dtype"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nxrJdRcnJO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = torch.from_numpy(numpy_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoFu-RdyBlbp",
        "colab_type": "code",
        "outputId": "dc4ba07e-c97e-43b2-a39d-724c9a0e97db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "input"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[133., 180.,  53.,  13.,  90., 113.],\n",
              "        [ 55.,  67.,  98.,  99.,  23., 101.],\n",
              "        [ 34.,  54.,  89.,   4.,  12.,   5.],\n",
              "        [ 56.,   2.,   3., 112.,  45., 156.],\n",
              "        [ 34.,   3., 220., 199., 200., 230.],\n",
              "        [ 67.,   1.,  45., 212., 240., 245.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pkOjEahKk4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = input.unsqueeze(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbI_qxKOKngJ",
        "colab_type": "code",
        "outputId": "f6ac892b-3352-45a1-ed85-5672e094e917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of5mxQWRJ3Cn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = torch.linspace(-1, 1, 8)\n",
        "meshx, meshy = torch.meshgrid((d, d))\n",
        "grid = torch.stack((meshy, meshx), 2)\n",
        "grid = grid.unsqueeze(0) # add batch dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_KztdLcKRSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = torch.nn.functional.grid_sample(input.unsqueeze(0), grid, align_corners=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjG7sTFjKTK7",
        "colab_type": "code",
        "outputId": "674902bb-92c6-4ddb-cf01-13a8c0a9519e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "print(test[0][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[133.0000, 166.5714, 125.5714,  47.2857,  18.7143,  57.0000,  96.5714,\n",
            "         113.0000],\n",
            "        [ 77.2857,  93.0000,  93.2245,  83.6122,  75.9592,  55.9796,  59.9388,\n",
            "         104.4286],\n",
            "        [ 46.0000,  57.0204,  75.4490,  89.0204,  63.4082,  35.4286,  30.1633,\n",
            "          59.8572],\n",
            "        [ 37.1429,  43.8776,  59.4898,  68.5306,  27.6122,  17.8775,  19.5306,\n",
            "          26.5714],\n",
            "        [ 52.8571,  21.8367,  11.9388,  26.8979,  84.9592,  64.4082,  67.1837,\n",
            "         134.4286],\n",
            "        [ 43.4286,  14.2449,  55.8980, 131.9592, 156.7551, 145.6327, 152.0612,\n",
            "         198.2857],\n",
            "        [ 43.4286,  14.1429,  74.2449, 174.6735, 198.0408, 207.6939, 217.9592,\n",
            "         234.2857],\n",
            "        [ 67.0000,  19.8571,  19.8571,  68.8571, 188.1429, 228.0000, 241.4286,\n",
            "         245.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}